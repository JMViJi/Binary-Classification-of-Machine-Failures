{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data import\n",
    "\n",
    "FE = 0 # Feature Engineering (0 = No, 1 = Yes)\n",
    "\n",
    "FR = 0 # Feature Reduction (0 = No, 1 = Yes)\n",
    "\n",
    "OR = 0  # Original not included\n",
    "\n",
    "CAT = 0 # Categorical data (0 = No, 1 = Yes)\n",
    "\n",
    "def dataset(FE, FR, OR, CAT):\n",
    "    if CAT == 0:\n",
    "        if OR == 0:\n",
    "            if FR == 0:\n",
    "                if FE == 0:\n",
    "                    X_train = pd.read_csv('data/X_train.csv')\n",
    "                    X_test = pd.read_csv('data/X_test.csv') # Test data\n",
    "                else:\n",
    "                    X_train = pd.read_csv('data/X_train_FE.csv')\n",
    "                    X_test = pd.read_csv('data/X_test_FE.csv') # Test data\n",
    "            else:\n",
    "                X_train = pd.read_csv('data/X_train_FR.csv')\n",
    "                X_test = pd.read_csv('data/X_test_FR.csv') # Test data\n",
    "        else:\n",
    "            if FE == 0:\n",
    "                X_train = pd.read_csv('data/X_total.csv')\n",
    "                X_test = pd.read_csv('data/X_test.csv') # Test data\n",
    "            else:\n",
    "                X_train = pd.read_csv('data/X_total_FE.csv')\n",
    "                X_test = pd.read_csv('data/X_test_FE.csv')\n",
    "    else:\n",
    "        X_train = pd.read_csv('data/X_train_CAT.csv')\n",
    "        X_test = pd.read_csv('data/X_test_CAT.csv') # Test data\n",
    "    \n",
    "    return X_train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def ROC_AUC(model, X_train, y_train, n_splits=3, n_repeats=3):\n",
    "    \n",
    "    # Initialize the Repeated Stratified K Fold\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
    "\n",
    "    # Create an empty list to store the Out-of-Fold (OOF) predictions\n",
    "    oof_preds = np.zeros(X_train.shape[0])\n",
    "\n",
    "    # Loop through each split\n",
    "    for train_index, valid_index in rskf.split(X_train, y_train):\n",
    "    \n",
    "        # Split the data\n",
    "        X_train_fold, X_valid_fold = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "        y_train_fold, y_valid_fold = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "    \n",
    "        # Fit the model\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "        # Make predictions and add them to the OOF predictions\n",
    "        oof_preds[valid_index] = model.predict_proba(X_valid_fold)[:,1]\n",
    "\n",
    "    # Calculate the overall AUC\n",
    "    auc = roc_auc_score(y_train, oof_preds)\n",
    "\n",
    "    return auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training datasets performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_LGBM = {'colsample_bytree': 0.1923023355270077,\n",
    "         'learning_rate': 0.03574583615481279,\n",
    "         'max_depth': 16, \n",
    "         'min_child_samples': 89, \n",
    "         'n_estimators': 260, \n",
    "         'num_leaves': 68, \n",
    "         'reg_alpha': 0.30028296727692755, \n",
    "         'reg_lambda': 0.6125642241926401, \n",
    "         'subsample': 0.7293703101825368\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_XGB =  {'max_depth': 13, \n",
    "         'learning_rate': 0.012308520402322306, \n",
    "         'colsample_bytree': 0.15564433647290904, \n",
    "         'subsample': 0.9392376085401448, \n",
    "         'n_estimators': 494, \n",
    "         'min_child_weight': 1, \n",
    "         'reg_alpha': 0.26760253520809857, \n",
    "         'reg_lambda': 0.24616802866656362} # best value: 0.9743947392021206."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_Cat_Opt_AllCat = {'iterations': 249, \n",
    "                       'depth': 6, \n",
    "                       'learning_rate': 0.1555748471212781, \n",
    "                       'random_strength': 58, \n",
    "                       'bagging_temperature': 87.47376677399185, \n",
    "                       'od_type': 'IncToDec', \n",
    "                       'od_wait': 27}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best =  {'colsample_bytree': 0.19297355677628159, \n",
    "          'learning_rate': 0.020755882048032786, \n",
    "          'max_depth': 9, \n",
    "          'min_child_samples': 41, \n",
    "          'n_estimators': 320, \n",
    "          'num_leaves': 100, \n",
    "          'reg_alpha': 0.39149507035237485, \n",
    "          'reg_lambda': 0.43245778149146746, \n",
    "          'subsample': 0.5188437264047947} # best loss: -0.973283159101125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_sets = {'Normal dataset': [0, 0, 0, 0], 'Dataset With FE': [1, 0, 0, 0], 'Dataset With FE and FR': [1, 1, 0, 0], 'Dataset Ori + train no FE': [0, 0, 1, 0], 'Dataset Ori + train with FE': [1, 0, 1, 0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136429, 14)\n",
      "Model:  Normal dataset    AUC:  0.9706861467455258\n",
      "(136429, 22)\n",
      "Model:  Dataset With FE    AUC:  0.9672851967906262\n",
      "(136429, 19)\n",
      "Model:  Dataset With FE and FR    AUC:  0.9653824357485317\n",
      "(146429, 14)\n",
      "Model:  Dataset Ori + train no FE    AUC:  0.9752747857937709\n",
      "(146429, 22)\n",
      "Model:  Dataset Ori + train with FE    AUC:  0.9726090433142951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Define the model with the best parameters\n",
    "model_LGBM = lgb.LGBMClassifier(**Best)\n",
    "\n",
    "Scores = {}\n",
    "\n",
    "for name, model in Train_sets.items():\n",
    "\n",
    "    X_train, X_test  = dataset(model[0], model[1], model[2], model[3])\n",
    "    y_train = X_train['Machine_failure']\n",
    "    X_train = X_train.drop(['Machine_failure'], axis=1)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # Initialize the Repeated Stratified K Fold\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=42)\n",
    "\n",
    "    # Create an empty list to store the Out-of-Fold (OOF) predictions\n",
    "    oof_preds = np.zeros(X_train.shape[0])\n",
    "\n",
    "    # Loop through each split\n",
    "    for train_index, valid_index in rskf.split(X_train, y_train):\n",
    "        \n",
    "        # Split the data\n",
    "        X_train_fold, X_valid_fold = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "        y_train_fold, y_valid_fold = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "        \n",
    "        # Fit the model\n",
    "        model_LGBM.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Make predictions and add them to the OOF predictions\n",
    "        oof_preds[valid_index] = model_LGBM.predict_proba(X_valid_fold)[:,1]\n",
    "\n",
    "    # Calculate the overall AUC\n",
    "    auc = roc_auc_score(y_train, oof_preds)\n",
    "\n",
    "    print(\"Model: \", name, \"   AUC: \", auc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test  = dataset(0, 0, 1, 0)\n",
    "y_train = X_train['Machine_failure']\n",
    "X_train = X_train.drop(['Machine_failure'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM hyper-parameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [13:11<00:00, 15.83s/trial, best loss: -0.973283159101125] \n",
      "Best:  {'colsample_bytree': 0.19297355677628159, 'learning_rate': 0.020755882048032786, 'max_depth': 9.0, 'min_child_samples': 41.0, 'n_estimators': 320.0, 'num_leaves': 100.0, 'reg_alpha': 0.39149507035237485, 'reg_lambda': 0.43245778149146746, 'subsample': 0.5188437264047947}\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import lightgbm as lgb\n",
    "# hp \n",
    "from hyperopt import hp, tpe, STATUS_OK, Trials, fmin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "\n",
    "space ={\n",
    "    'max_depth': hp.quniform('max_depth', 6, 18, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 100, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.01, 0.3),\n",
    "    'subsample': hp.uniform('subsample', 0.4, 1.0),\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 400, 10),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 100, 1),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.15, 0.5),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.3, 1.0)\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    model_LGBM = lgb.LGBMClassifier(max_depth = int(space['max_depth']),\n",
    "                             learning_rate = space['learning_rate'],\n",
    "                             num_leaves = int(space['num_leaves']),\n",
    "                             n_estimators = int(space['n_estimators']),\n",
    "                             colsample_bytree = space['colsample_bytree'],\n",
    "                             subsample = space['subsample'],\n",
    "                             is_unbalance = True,\n",
    "                             min_child_samples = int(space['min_child_samples']),\n",
    "                             reg_alpha = space['reg_alpha'],\n",
    "                             reg_lambda = space['reg_lambda'])\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "    y_pred_proba = cross_val_predict(model_LGBM, X_train, y_train, cv=cv, method='predict_proba')[:,1]\n",
    "    auc = roc_auc_score(y_train, y_pred_proba)\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n",
    "\n",
    "# Run the algorithm\n",
    "trials = Trials()\n",
    "best_LGBM = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Best: \", best_LGBM)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_LGBM =  {'colsample_bytree': 0.19297355677628159, \n",
    "          'learning_rate': 0.020755882048032786, \n",
    "          'max_depth': 9, \n",
    "          'min_child_samples': 41, \n",
    "          'n_estimators': 320, \n",
    "          'num_leaves': 100, \n",
    "          'reg_alpha': 0.39149507035237485, \n",
    "          'reg_lambda': 0.43245778149146746, \n",
    "          'subsample': 0.5188437264047947} # best loss: -0.973283159101125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LGBM = lgb.LGBMClassifier(**Best_LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC LGBM:  0.9752747857937709\n"
     ]
    }
   ],
   "source": [
    "AUC_LGBM = ROC_AUC(model_LGBM, X_train, y_train)\n",
    "print(\"AUC LGBM: \", AUC_LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LGBM.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-17 05:21:02,743]\u001b[0m A new study created in memory with name: no-name-7add8121-9694-4e28-8a09-f07f15c515d8\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:21:09,176]\u001b[0m Trial 0 finished with value: 0.9616607567456765 and parameters: {'max_depth': 4, 'learning_rate': 0.116001680777605, 'colsample_bytree': 0.14692953139998594, 'subsample': 0.8030857175987267, 'n_estimators': 261, 'min_child_weight': 10, 'reg_alpha': 0.16187385783406927, 'reg_lambda': 0.260216248238337}. Best is trial 0 with value: 0.9616607567456765.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:21:33,296]\u001b[0m Trial 1 finished with value: 0.9682791249913572 and parameters: {'max_depth': 16, 'learning_rate': 0.026303751565074036, 'colsample_bytree': 0.275439983412078, 'subsample': 0.8493748844170721, 'n_estimators': 519, 'min_child_weight': 5, 'reg_alpha': 0.3696863546810296, 'reg_lambda': 0.4048009875593822}. Best is trial 1 with value: 0.9682791249913572.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:21:43,373]\u001b[0m Trial 2 finished with value: 0.9598617173352173 and parameters: {'max_depth': 11, 'learning_rate': 0.011227688917877333, 'colsample_bytree': 0.11396595445019367, 'subsample': 0.6250640262575275, 'n_estimators': 419, 'min_child_weight': 6, 'reg_alpha': 0.12582089741731273, 'reg_lambda': 0.7504594384526924}. Best is trial 1 with value: 0.9682791249913572.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:21:48,882]\u001b[0m Trial 3 finished with value: 0.9658059898022322 and parameters: {'max_depth': 4, 'learning_rate': 0.09186769786316298, 'colsample_bytree': 0.20099331972860202, 'subsample': 0.6568651682594007, 'n_estimators': 231, 'min_child_weight': 3, 'reg_alpha': 0.42621438075239537, 'reg_lambda': 0.7711950788117641}. Best is trial 1 with value: 0.9682791249913572.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:22:04,753]\u001b[0m Trial 4 finished with value: 0.9685422499694137 and parameters: {'max_depth': 16, 'learning_rate': 0.049266729871218884, 'colsample_bytree': 0.12510788344936813, 'subsample': 0.9659287993042049, 'n_estimators': 505, 'min_child_weight': 2, 'reg_alpha': 0.12906879029225465, 'reg_lambda': 0.9006375513035179}. Best is trial 4 with value: 0.9685422499694137.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:22:09,873]\u001b[0m Trial 5 finished with value: 0.9637939101115002 and parameters: {'max_depth': 12, 'learning_rate': 0.03144446664977247, 'colsample_bytree': 0.11473882373713054, 'subsample': 0.7327831695611223, 'n_estimators': 212, 'min_child_weight': 6, 'reg_alpha': 0.4885900974210745, 'reg_lambda': 0.7915749521687394}. Best is trial 4 with value: 0.9685422499694137.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:22:20,660]\u001b[0m Trial 6 finished with value: 0.957428336951363 and parameters: {'max_depth': 7, 'learning_rate': 0.09869176092859104, 'colsample_bytree': 0.137407296570532, 'subsample': 0.9136100053562279, 'n_estimators': 493, 'min_child_weight': 7, 'reg_alpha': 0.3996321155896304, 'reg_lambda': 0.9031700754652814}. Best is trial 4 with value: 0.9685422499694137.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:22:28,549]\u001b[0m Trial 7 finished with value: 0.9680556692454968 and parameters: {'max_depth': 14, 'learning_rate': 0.12956390590202427, 'colsample_bytree': 0.1065061180907273, 'subsample': 0.7673228869309021, 'n_estimators': 235, 'min_child_weight': 1, 'reg_alpha': 0.49086996176969233, 'reg_lambda': 0.5641848734156203}. Best is trial 4 with value: 0.9685422499694137.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:22:39,848]\u001b[0m Trial 8 finished with value: 0.9649211519915691 and parameters: {'max_depth': 13, 'learning_rate': 0.011216283142506937, 'colsample_bytree': 0.17599900133855212, 'subsample': 0.6995167438412438, 'n_estimators': 309, 'min_child_weight': 4, 'reg_alpha': 0.29388289976768706, 'reg_lambda': 0.5283293121074549}. Best is trial 4 with value: 0.9685422499694137.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:22:50,294]\u001b[0m Trial 9 finished with value: 0.9605596513273752 and parameters: {'max_depth': 16, 'learning_rate': 0.012813266407105499, 'colsample_bytree': 0.13201076735432588, 'subsample': 0.768570142849882, 'n_estimators': 362, 'min_child_weight': 8, 'reg_alpha': 0.12811971735972066, 'reg_lambda': 0.30878033673050664}. Best is trial 4 with value: 0.9685422499694137.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:23:19,722]\u001b[0m Trial 10 finished with value: 0.9653076488929924 and parameters: {'max_depth': 18, 'learning_rate': 0.055951433125711485, 'colsample_bytree': 0.19250450406647326, 'subsample': 0.9998393465114278, 'n_estimators': 597, 'min_child_weight': 1, 'reg_alpha': 0.08013610704524293, 'reg_lambda': 0.9822810718182585}. Best is trial 4 with value: 0.9685422499694137.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:23:49,310]\u001b[0m Trial 11 finished with value: 0.9682470228638539 and parameters: {'max_depth': 18, 'learning_rate': 0.02919539058320176, 'colsample_bytree': 0.2675758700923586, 'subsample': 0.5237184477066004, 'n_estimators': 528, 'min_child_weight': 3, 'reg_alpha': 0.2626249380925545, 'reg_lambda': 0.4417309655456875}. Best is trial 4 with value: 0.9685422499694137.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:24:21,274]\u001b[0m Trial 12 finished with value: 0.9640448714883132 and parameters: {'max_depth': 15, 'learning_rate': 0.04440365138292265, 'colsample_bytree': 0.29694158346198823, 'subsample': 0.8802090329678628, 'n_estimators': 480, 'min_child_weight': 4, 'reg_alpha': 0.0518405570709969, 'reg_lambda': 0.40864949403857226}. Best is trial 4 with value: 0.9685422499694137.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:24:51,090]\u001b[0m Trial 13 finished with value: 0.9699223847328247 and parameters: {'max_depth': 10, 'learning_rate': 0.01961478056894963, 'colsample_bytree': 0.2283921410213412, 'subsample': 0.8783617157255006, 'n_estimators': 591, 'min_child_weight': 2, 'reg_alpha': 0.22703725784906376, 'reg_lambda': 0.6078521247881186}. Best is trial 13 with value: 0.9699223847328247.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:25:23,160]\u001b[0m Trial 14 finished with value: 0.9699888383372066 and parameters: {'max_depth': 10, 'learning_rate': 0.019980502785002453, 'colsample_bytree': 0.2289113170345272, 'subsample': 0.9592070229645258, 'n_estimators': 598, 'min_child_weight': 2, 'reg_alpha': 0.19918438499574503, 'reg_lambda': 0.67710694360993}. Best is trial 14 with value: 0.9699888383372066.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:25:49,340]\u001b[0m Trial 15 finished with value: 0.9703924743028949 and parameters: {'max_depth': 9, 'learning_rate': 0.017582517139833262, 'colsample_bytree': 0.2253979756624831, 'subsample': 0.9294704936770903, 'n_estimators': 598, 'min_child_weight': 2, 'reg_alpha': 0.224468508376753, 'reg_lambda': 0.6560291214342387}. Best is trial 15 with value: 0.9703924743028949.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:26:12,862]\u001b[0m Trial 16 finished with value: 0.9703449323478835 and parameters: {'max_depth': 9, 'learning_rate': 0.017634963986507036, 'colsample_bytree': 0.23908079409784516, 'subsample': 0.9508176135477039, 'n_estimators': 550, 'min_child_weight': 3, 'reg_alpha': 0.1935030340494969, 'reg_lambda': 0.6572706626396185}. Best is trial 15 with value: 0.9703924743028949.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:26:28,879]\u001b[0m Trial 17 finished with value: 0.9697378525109438 and parameters: {'max_depth': 8, 'learning_rate': 0.019539733960064292, 'colsample_bytree': 0.2364735279708512, 'subsample': 0.9183206473299155, 'n_estimators': 441, 'min_child_weight': 4, 'reg_alpha': 0.3082142089541953, 'reg_lambda': 0.6644712337095293}. Best is trial 15 with value: 0.9703924743028949.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:26:50,886]\u001b[0m Trial 18 finished with value: 0.9698978374852899 and parameters: {'max_depth': 7, 'learning_rate': 0.01473854493650028, 'colsample_bytree': 0.24684319660540033, 'subsample': 0.8129309780921968, 'n_estimators': 549, 'min_child_weight': 3, 'reg_alpha': 0.21574909572001377, 'reg_lambda': 0.5057848513646727}. Best is trial 15 with value: 0.9703924743028949.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:27:07,794]\u001b[0m Trial 19 finished with value: 0.971453617095603 and parameters: {'max_depth': 8, 'learning_rate': 0.015348061859317369, 'colsample_bytree': 0.2114886454584332, 'subsample': 0.9925971802632751, 'n_estimators': 561, 'min_child_weight': 1, 'reg_alpha': 0.2537699933517125, 'reg_lambda': 0.6235452837199065}. Best is trial 19 with value: 0.971453617095603.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:27:17,206]\u001b[0m Trial 20 finished with value: 0.9665964038380159 and parameters: {'max_depth': 6, 'learning_rate': 0.010218767541068197, 'colsample_bytree': 0.20819564961336612, 'subsample': 0.9955106395853048, 'n_estimators': 373, 'min_child_weight': 1, 'reg_alpha': 0.26053045667984503, 'reg_lambda': 0.46994729744763586}. Best is trial 19 with value: 0.971453617095603.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:27:41,388]\u001b[0m Trial 21 finished with value: 0.9714194734390468 and parameters: {'max_depth': 9, 'learning_rate': 0.01567374256820468, 'colsample_bytree': 0.21823652406709346, 'subsample': 0.9265680303598691, 'n_estimators': 555, 'min_child_weight': 2, 'reg_alpha': 0.1918487446652864, 'reg_lambda': 0.6173234027704728}. Best is trial 19 with value: 0.971453617095603.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:28:04,502]\u001b[0m Trial 22 finished with value: 0.9718928514601064 and parameters: {'max_depth': 9, 'learning_rate': 0.015141499558941875, 'colsample_bytree': 0.2156053623480289, 'subsample': 0.9187313688984133, 'n_estimators': 555, 'min_child_weight': 1, 'reg_alpha': 0.248357500846742, 'reg_lambda': 0.6116762609941473}. Best is trial 22 with value: 0.9718928514601064.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:28:17,619]\u001b[0m Trial 23 finished with value: 0.967979594782644 and parameters: {'max_depth': 6, 'learning_rate': 0.01413397987559009, 'colsample_bytree': 0.1745960769559239, 'subsample': 0.8770264096331677, 'n_estimators': 550, 'min_child_weight': 1, 'reg_alpha': 0.31162699726254994, 'reg_lambda': 0.5796318657407356}. Best is trial 22 with value: 0.9718928514601064.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:28:32,642]\u001b[0m Trial 24 finished with value: 0.9733636203336882 and parameters: {'max_depth': 11, 'learning_rate': 0.023504775422822144, 'colsample_bytree': 0.2133991704643525, 'subsample': 0.9999315828217257, 'n_estimators': 462, 'min_child_weight': 1, 'reg_alpha': 0.251049527443117, 'reg_lambda': 0.5417510711783408}. Best is trial 24 with value: 0.9733636203336882.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:28:48,650]\u001b[0m Trial 25 finished with value: 0.9737589923853656 and parameters: {'max_depth': 12, 'learning_rate': 0.022205921041809882, 'colsample_bytree': 0.210226897501822, 'subsample': 0.9869930958984228, 'n_estimators': 456, 'min_child_weight': 1, 'reg_alpha': 0.26053524569159003, 'reg_lambda': 0.5295124036396301}. Best is trial 25 with value: 0.9737589923853656.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:29:01,965]\u001b[0m Trial 26 finished with value: 0.9675770590421936 and parameters: {'max_depth': 11, 'learning_rate': 0.02373734463218974, 'colsample_bytree': 0.19493218504877535, 'subsample': 0.954127826789085, 'n_estimators': 428, 'min_child_weight': 10, 'reg_alpha': 0.33661425212396245, 'reg_lambda': 0.5402267405380681}. Best is trial 25 with value: 0.9737589923853656.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:29:27,448]\u001b[0m Trial 27 finished with value: 0.9673599234790902 and parameters: {'max_depth': 13, 'learning_rate': 0.03528806497019343, 'colsample_bytree': 0.24906954853663302, 'subsample': 0.9973213678556078, 'n_estimators': 462, 'min_child_weight': 8, 'reg_alpha': 0.2807892042189291, 'reg_lambda': 0.49624754162084894}. Best is trial 25 with value: 0.9737589923853656.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:29:48,248]\u001b[0m Trial 28 finished with value: 0.9719666887983085 and parameters: {'max_depth': 12, 'learning_rate': 0.02457330622706314, 'colsample_bytree': 0.21439898222062798, 'subsample': 0.90250326307129, 'n_estimators': 387, 'min_child_weight': 1, 'reg_alpha': 0.24492320429236156, 'reg_lambda': 0.38104864614851386}. Best is trial 25 with value: 0.9737589923853656.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:30:01,275]\u001b[0m Trial 29 finished with value: 0.970356924802979 and parameters: {'max_depth': 12, 'learning_rate': 0.024776960351924972, 'colsample_bytree': 0.18471024059468574, 'subsample': 0.8364980768459018, 'n_estimators': 317, 'min_child_weight': 5, 'reg_alpha': 0.17117621398663171, 'reg_lambda': 0.32639810088681614}. Best is trial 25 with value: 0.9737589923853656.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:30:14,889]\u001b[0m Trial 30 finished with value: 0.9661329522935491 and parameters: {'max_depth': 13, 'learning_rate': 0.03602887880768253, 'colsample_bytree': 0.1594291671388915, 'subsample': 0.8926599485187745, 'n_estimators': 381, 'min_child_weight': 10, 'reg_alpha': 0.33127249578393614, 'reg_lambda': 0.26102864525256997}. Best is trial 25 with value: 0.9737589923853656.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:30:33,775]\u001b[0m Trial 31 finished with value: 0.9739804432763476 and parameters: {'max_depth': 12, 'learning_rate': 0.022589400185046914, 'colsample_bytree': 0.21046611639919174, 'subsample': 0.9522285284545021, 'n_estimators': 456, 'min_child_weight': 1, 'reg_alpha': 0.2374848219663962, 'reg_lambda': 0.2010921546529738}. Best is trial 31 with value: 0.9739804432763476.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:30:53,138]\u001b[0m Trial 32 finished with value: 0.9725224736897026 and parameters: {'max_depth': 12, 'learning_rate': 0.023112703649386696, 'colsample_bytree': 0.2007270475496867, 'subsample': 0.9631499241363219, 'n_estimators': 456, 'min_child_weight': 2, 'reg_alpha': 0.28303534295995897, 'reg_lambda': 0.20194678608554673}. Best is trial 31 with value: 0.9739804432763476.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:31:13,165]\u001b[0m Trial 33 finished with value: 0.9728451453025908 and parameters: {'max_depth': 14, 'learning_rate': 0.021315338767939094, 'colsample_bytree': 0.2036311528119902, 'subsample': 0.9653728298940094, 'n_estimators': 460, 'min_child_weight': 2, 'reg_alpha': 0.2922138576000092, 'reg_lambda': 0.21328587609875396}. Best is trial 31 with value: 0.9739804432763476.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:31:28,920]\u001b[0m Trial 34 finished with value: 0.9708958273493304 and parameters: {'max_depth': 14, 'learning_rate': 0.02771351410807942, 'colsample_bytree': 0.20176741863936726, 'subsample': 0.9705616817910606, 'n_estimators': 408, 'min_child_weight': 3, 'reg_alpha': 0.28030544878931357, 'reg_lambda': 0.2159765405659666}. Best is trial 31 with value: 0.9739804432763476.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:31:49,206]\u001b[0m Trial 35 finished with value: 0.9717718266839143 and parameters: {'max_depth': 14, 'learning_rate': 0.022115503558708945, 'colsample_bytree': 0.18919173276130516, 'subsample': 0.8562156337470487, 'n_estimators': 477, 'min_child_weight': 2, 'reg_alpha': 0.34269180515770614, 'reg_lambda': 0.3508081798856395}. Best is trial 31 with value: 0.9739804432763476.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:32:09,388]\u001b[0m Trial 36 finished with value: 0.9718317522852167 and parameters: {'max_depth': 15, 'learning_rate': 0.03026890861935013, 'colsample_bytree': 0.2045318596835776, 'subsample': 0.9484431350372043, 'n_estimators': 440, 'min_child_weight': 1, 'reg_alpha': 0.23266509292031365, 'reg_lambda': 0.27351718325931224}. Best is trial 31 with value: 0.9739804432763476.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:32:29,231]\u001b[0m Trial 37 finished with value: 0.970739852084741 and parameters: {'max_depth': 11, 'learning_rate': 0.026523427897460646, 'colsample_bytree': 0.18172653533736396, 'subsample': 0.9760143386446761, 'n_estimators': 513, 'min_child_weight': 2, 'reg_alpha': 0.3637693533094015, 'reg_lambda': 0.42976559888298227}. Best is trial 31 with value: 0.9739804432763476.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:32:44,419]\u001b[0m Trial 38 finished with value: 0.970824532753901 and parameters: {'max_depth': 17, 'learning_rate': 0.031667584074253, 'colsample_bytree': 0.16678390908150897, 'subsample': 0.9367627512944233, 'n_estimators': 351, 'min_child_weight': 3, 'reg_alpha': 0.3059277562361704, 'reg_lambda': 0.3754040074724194}. Best is trial 31 with value: 0.9739804432763476.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:32:58,414]\u001b[0m Trial 39 finished with value: 0.9709506185661883 and parameters: {'max_depth': 10, 'learning_rate': 0.022188307282348507, 'colsample_bytree': 0.19559986365854337, 'subsample': 0.9760130005547423, 'n_estimators': 406, 'min_child_weight': 5, 'reg_alpha': 0.16117347293356615, 'reg_lambda': 0.3032859382891306}. Best is trial 31 with value: 0.9739804432763476.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:33:20,106]\u001b[0m Trial 40 finished with value: 0.9743403025022838 and parameters: {'max_depth': 15, 'learning_rate': 0.012262206499556913, 'colsample_bytree': 0.15610674442826583, 'subsample': 0.90232090833853, 'n_estimators': 492, 'min_child_weight': 1, 'reg_alpha': 0.265915658610763, 'reg_lambda': 0.2335190358321967}. Best is trial 40 with value: 0.9743403025022838.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:33:40,978]\u001b[0m Trial 41 finished with value: 0.9743947392021206 and parameters: {'max_depth': 13, 'learning_rate': 0.012308520402322306, 'colsample_bytree': 0.15564433647290904, 'subsample': 0.9392376085401448, 'n_estimators': 494, 'min_child_weight': 1, 'reg_alpha': 0.26760253520809857, 'reg_lambda': 0.24616802866656362}. Best is trial 41 with value: 0.9743947392021206.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:34:00,903]\u001b[0m Trial 42 finished with value: 0.9736227722761626 and parameters: {'max_depth': 15, 'learning_rate': 0.010014370508003719, 'colsample_bytree': 0.15361439055049764, 'subsample': 0.9066131633180807, 'n_estimators': 492, 'min_child_weight': 1, 'reg_alpha': 0.26881904299604925, 'reg_lambda': 0.2691819922374301}. Best is trial 41 with value: 0.9743947392021206.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:34:23,576]\u001b[0m Trial 43 finished with value: 0.9743460603476966 and parameters: {'max_depth': 15, 'learning_rate': 0.011982087391230046, 'colsample_bytree': 0.15082632039597196, 'subsample': 0.9011169503461768, 'n_estimators': 485, 'min_child_weight': 1, 'reg_alpha': 0.2648922782919986, 'reg_lambda': 0.2516882937883905}. Best is trial 41 with value: 0.9743947392021206.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:34:39,036]\u001b[0m Trial 44 finished with value: 0.9712582415427642 and parameters: {'max_depth': 16, 'learning_rate': 0.012287508222693724, 'colsample_bytree': 0.14104005135815179, 'subsample': 0.8537395330288496, 'n_estimators': 529, 'min_child_weight': 1, 'reg_alpha': 0.27133201312081856, 'reg_lambda': 0.23292161168915354}. Best is trial 41 with value: 0.9743947392021206.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:34:58,566]\u001b[0m Trial 45 finished with value: 0.9726807227531176 and parameters: {'max_depth': 13, 'learning_rate': 0.012478529245199082, 'colsample_bytree': 0.146599204502653, 'subsample': 0.8941007155003987, 'n_estimators': 505, 'min_child_weight': 2, 'reg_alpha': 0.23848244659934983, 'reg_lambda': 0.24513778926681096}. Best is trial 41 with value: 0.9743947392021206.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:35:11,286]\u001b[0m Trial 46 finished with value: 0.9614618237979012 and parameters: {'max_depth': 17, 'learning_rate': 0.011389198578564354, 'colsample_bytree': 0.12380783516264063, 'subsample': 0.9327384835600258, 'n_estimators': 497, 'min_child_weight': 7, 'reg_alpha': 0.30328898983921554, 'reg_lambda': 0.2877335875975264}. Best is trial 41 with value: 0.9743947392021206.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:35:32,840]\u001b[0m Trial 47 finished with value: 0.9723102157918214 and parameters: {'max_depth': 15, 'learning_rate': 0.012642193201924987, 'colsample_bytree': 0.1658637774154375, 'subsample': 0.93928039162306, 'n_estimators': 478, 'min_child_weight': 3, 'reg_alpha': 0.20961578883714893, 'reg_lambda': 0.32853906251104564}. Best is trial 41 with value: 0.9743947392021206.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:35:55,472]\u001b[0m Trial 48 finished with value: 0.9713775059585754 and parameters: {'max_depth': 14, 'learning_rate': 0.017122117622042726, 'colsample_bytree': 0.150891533104401, 'subsample': 0.9094291400189296, 'n_estimators': 521, 'min_child_weight': 4, 'reg_alpha': 0.23093626364627254, 'reg_lambda': 0.23988583249052187}. Best is trial 41 with value: 0.9743947392021206.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 05:36:07,065]\u001b[0m Trial 49 finished with value: 0.9699110401881472 and parameters: {'max_depth': 17, 'learning_rate': 0.01076632830343358, 'colsample_bytree': 0.13484966635084109, 'subsample': 0.8799567814651718, 'n_estimators': 424, 'min_child_weight': 1, 'reg_alpha': 0.265439383808042, 'reg_lambda': 0.30198991620860677}. Best is trial 41 with value: 0.9743947392021206.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best:  {'max_depth': 13, 'learning_rate': 0.012308520402322306, 'colsample_bytree': 0.15564433647290904, 'subsample': 0.9392376085401448, 'n_estimators': 494, 'min_child_weight': 1, 'reg_alpha': 0.26760253520809857, 'reg_lambda': 0.24616802866656362}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 18),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.05, 0.5),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.2, 1.0),\n",
    "    }\n",
    "    \n",
    "    model_XGB = xgb.XGBClassifier(**params, n_jobs = -1)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    y_pred_proba = cross_val_predict(model_XGB, X_train, y_train, cv=cv, method='predict_proba')[:,1]\n",
    "    auc = roc_auc_score(y_train, y_pred_proba)\n",
    "    \n",
    "    return auc\n",
    "\n",
    "# Create a study object\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Start optimization\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get the best parameters\n",
    "best_XGB = study.best_params\n",
    "print(\"Best: \", best_XGB)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_XGB =  {'max_depth': 13, \n",
    "         'learning_rate': 0.012308520402322306, \n",
    "         'colsample_bytree': 0.15564433647290904, \n",
    "         'subsample': 0.9392376085401448, \n",
    "         'n_estimators': 494, \n",
    "         'min_child_weight': 1, \n",
    "         'reg_alpha': 0.26760253520809857, \n",
    "         'reg_lambda': 0.24616802866656362} # best value: 0.9743947392021206."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.15564433647290904, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.012308520402322306,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=494, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.15564433647290904, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.012308520402322306,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=494, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.15564433647290904, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.012308520402322306,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=494, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model_XGB = xgb.XGBClassifier(**Best_XGB)\n",
    "model_XGB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC XGB:  0.9784126460666145\n"
     ]
    }
   ],
   "source": [
    "AUC_XGB = ROC_AUC(model_XGB, X_train, y_train)\n",
    "print(\"AUC XGB: \", AUC_XGB)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_CAT, X_test_CAT = dataset(0,0,0,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_CAT = X_train_CAT['Machine_failure']\n",
    "X_train_CAT = X_train_CAT.drop('Machine_failure', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product_ID               10000\n",
       "Type                         3\n",
       "Air_temperature_K           96\n",
       "Process_temperature_K       82\n",
       "Rotational_speed_rpm       957\n",
       "Torque_Nm                  615\n",
       "Tool_wear_min              246\n",
       "TWF                          2\n",
       "HDF                          2\n",
       "PWF                          2\n",
       "OSF                          2\n",
       "RNF                          2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total unique values in each column of X_train\n",
    "X_train_CAT.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ['Product_ID', 'Type', 'Air_temperature_K', 'Process_temperature_K','Rotational_speed_rpm', 'Torque_Nm', 'Tool_wear_min', 'TWF', 'HDF','PWF', 'OSF', 'RNF']\n",
    "\n",
    "# Convert cat columns of X_train to str\n",
    "for col in cat:\n",
    "    X_train_CAT[col] = X_train_CAT[col].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-17 11:41:53,370]\u001b[0m A new study created in memory with name: no-name-449c8501-3d31-496b-8116-78031ca34173\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:42:05,471]\u001b[0m Trial 0 finished with value: 0.9533153317183979 and parameters: {'iterations': 126, 'depth': 4, 'learning_rate': 0.013998335101610257, 'random_strength': 52, 'bagging_temperature': 7.60416911051556, 'od_type': 'IncToDec', 'od_wait': 44}. Best is trial 0 with value: 0.9533153317183979.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:43:20,402]\u001b[0m Trial 1 finished with value: 0.9746634364468246 and parameters: {'iterations': 233, 'depth': 7, 'learning_rate': 0.011270440591083359, 'random_strength': 31, 'bagging_temperature': 18.53878123731863, 'od_type': 'IncToDec', 'od_wait': 50}. Best is trial 1 with value: 0.9746634364468246.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:44:22,709]\u001b[0m Trial 2 finished with value: 0.9782274073884368 and parameters: {'iterations': 216, 'depth': 5, 'learning_rate': 0.030656105623185603, 'random_strength': 4, 'bagging_temperature': 0.0360011058383201, 'od_type': 'IncToDec', 'od_wait': 13}. Best is trial 2 with value: 0.9782274073884368.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:45:02,319]\u001b[0m Trial 3 finished with value: 0.9780196258997949 and parameters: {'iterations': 256, 'depth': 7, 'learning_rate': 0.1136592354090119, 'random_strength': 12, 'bagging_temperature': 1.8908491843658446, 'od_type': 'Iter', 'od_wait': 19}. Best is trial 2 with value: 0.9782274073884368.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:46:50,375]\u001b[0m Trial 4 finished with value: 0.9779070255138733 and parameters: {'iterations': 280, 'depth': 7, 'learning_rate': 0.012615220695055152, 'random_strength': 7, 'bagging_temperature': 91.89540988794793, 'od_type': 'Iter', 'od_wait': 48}. Best is trial 2 with value: 0.9782274073884368.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:47:30,443]\u001b[0m Trial 5 finished with value: 0.9731395238918804 and parameters: {'iterations': 277, 'depth': 7, 'learning_rate': 0.11077082423545258, 'random_strength': 68, 'bagging_temperature': 37.70265784792898, 'od_type': 'Iter', 'od_wait': 14}. Best is trial 2 with value: 0.9782274073884368.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:47:40,353]\u001b[0m Trial 6 finished with value: 0.9763864066958088 and parameters: {'iterations': 64, 'depth': 7, 'learning_rate': 0.1495059388793994, 'random_strength': 34, 'bagging_temperature': 0.06115254127562842, 'od_type': 'Iter', 'od_wait': 41}. Best is trial 2 with value: 0.9782274073884368.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:48:19,577]\u001b[0m Trial 7 finished with value: 0.9739174274428443 and parameters: {'iterations': 210, 'depth': 5, 'learning_rate': 0.06809993510741354, 'random_strength': 57, 'bagging_temperature': 0.11289545341447103, 'od_type': 'Iter', 'od_wait': 13}. Best is trial 2 with value: 0.9782274073884368.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:48:34,412]\u001b[0m Trial 8 finished with value: 0.9575128829445295 and parameters: {'iterations': 144, 'depth': 5, 'learning_rate': 0.02123863797222442, 'random_strength': 12, 'bagging_temperature': 20.562571371475926, 'od_type': 'IncToDec', 'od_wait': 30}. Best is trial 2 with value: 0.9782274073884368.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:50:06,471]\u001b[0m Trial 9 finished with value: 0.9766996407530973 and parameters: {'iterations': 299, 'depth': 6, 'learning_rate': 0.030389722807181028, 'random_strength': 61, 'bagging_temperature': 0.020426810724468402, 'od_type': 'IncToDec', 'od_wait': 19}. Best is trial 2 with value: 0.9782274073884368.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:50:25,241]\u001b[0m Trial 10 finished with value: 0.9738125135809463 and parameters: {'iterations': 195, 'depth': 4, 'learning_rate': 0.048409326480165465, 'random_strength': 86, 'bagging_temperature': 0.34721150274380846, 'od_type': 'IncToDec', 'od_wait': 28}. Best is trial 2 with value: 0.9782274073884368.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:51:23,335]\u001b[0m Trial 11 finished with value: 0.9783593516221193 and parameters: {'iterations': 248, 'depth': 8, 'learning_rate': 0.07556114402524425, 'random_strength': 2, 'bagging_temperature': 1.6353270643758222, 'od_type': 'Iter', 'od_wait': 22}. Best is trial 11 with value: 0.9783593516221193.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:51:44,118]\u001b[0m Trial 12 finished with value: 0.9755570352046471 and parameters: {'iterations': 164, 'depth': 8, 'learning_rate': 0.062330932099259655, 'random_strength': 28, 'bagging_temperature': 0.9397974794275534, 'od_type': 'IncToDec', 'od_wait': 23}. Best is trial 11 with value: 0.9783593516221193.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:51:49,580]\u001b[0m Trial 13 finished with value: 0.8960048072205504 and parameters: {'iterations': 241, 'depth': 5, 'learning_rate': 0.03259290691424029, 'random_strength': 0, 'bagging_temperature': 0.02018472480393091, 'od_type': 'Iter', 'od_wait': 11}. Best is trial 11 with value: 0.9783593516221193.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:53:14,036]\u001b[0m Trial 14 finished with value: 0.9783828769436803 and parameters: {'iterations': 200, 'depth': 8, 'learning_rate': 0.0773442705589561, 'random_strength': 22, 'bagging_temperature': 2.584649261741086, 'od_type': 'IncToDec', 'od_wait': 37}. Best is trial 14 with value: 0.9783828769436803.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:53:28,041]\u001b[0m Trial 15 finished with value: 0.9747359788877734 and parameters: {'iterations': 102, 'depth': 8, 'learning_rate': 0.08099073225438096, 'random_strength': 19, 'bagging_temperature': 1.9182002617629277, 'od_type': 'Iter', 'od_wait': 37}. Best is trial 14 with value: 0.9783828769436803.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:53:56,484]\u001b[0m Trial 16 finished with value: 0.977112076780036 and parameters: {'iterations': 176, 'depth': 8, 'learning_rate': 0.19943729399706578, 'random_strength': 40, 'bagging_temperature': 5.34485929767195, 'od_type': 'IncToDec', 'od_wait': 36}. Best is trial 14 with value: 0.9783828769436803.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:54:17,496]\u001b[0m Trial 17 finished with value: 0.9769099713161497 and parameters: {'iterations': 187, 'depth': 8, 'learning_rate': 0.09067945828980382, 'random_strength': 20, 'bagging_temperature': 0.3159785784935842, 'od_type': 'Iter', 'od_wait': 25}. Best is trial 14 with value: 0.9783828769436803.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:55:33,203]\u001b[0m Trial 18 finished with value: 0.9778382693700286 and parameters: {'iterations': 256, 'depth': 6, 'learning_rate': 0.05876630379081047, 'random_strength': 43, 'bagging_temperature': 4.350581375815126, 'od_type': 'IncToDec', 'od_wait': 35}. Best is trial 14 with value: 0.9783828769436803.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:56:34,929]\u001b[0m Trial 19 finished with value: 0.9767457650071071 and parameters: {'iterations': 218, 'depth': 8, 'learning_rate': 0.0478521563694663, 'random_strength': 94, 'bagging_temperature': 0.5538329805022981, 'od_type': 'Iter', 'od_wait': 34}. Best is trial 14 with value: 0.9783828769436803.\u001b[0m\n",
      "\u001b[32m[I 2023-06-17 11:56:53,765]\u001b[0m Trial 20 finished with value: 0.9764947060786741 and parameters: {'iterations': 157, 'depth': 6, 'learning_rate': 0.08392553130401237, 'random_strength': 23, 'bagging_temperature': 0.17467290366364477, 'od_type': 'Iter', 'od_wait': 31}. Best is trial 14 with value: 0.9783828769436803.\u001b[0m\n",
      "\u001b[33m[W 2023-06-17 11:57:17,293]\u001b[0m Trial 21 failed with parameters: {'iterations': 211, 'depth': 5, 'learning_rate': 0.03709756339973129, 'random_strength': 2, 'bagging_temperature': 1.2694495167204913, 'od_type': 'IncToDec', 'od_wait': 22} because of the following error: KeyboardInterrupt('').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jose\\AppData\\Local\\Temp\\ipykernel_11612\\541721198.py\", line 28, in objective\n",
      "    model_CAT.fit(train_pool, eval_set=valid_pool)\n",
      "  File \"c:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py\", line 5131, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"c:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py\", line 2357, in _fit\n",
      "    self._train(\n",
      "  File \"c:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py\", line 1761, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 4624, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 4673, in _catboost._CatBoost._train\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-06-17 11:57:17,294]\u001b[0m Trial 21 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[39m# Start optimization\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     _optimize(\n\u001b[0;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    435\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[43], line 28\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     25\u001b[0m     train_pool \u001b[39m=\u001b[39m Pool(data\u001b[39m=\u001b[39mX_train\u001b[39m.\u001b[39miloc[train_index], label\u001b[39m=\u001b[39my_train\u001b[39m.\u001b[39miloc[train_index], cat_features \u001b[39m=\u001b[39m cat)\n\u001b[0;32m     26\u001b[0m     valid_pool \u001b[39m=\u001b[39m Pool(data\u001b[39m=\u001b[39mX_train\u001b[39m.\u001b[39miloc[valid_index], label\u001b[39m=\u001b[39my_train\u001b[39m.\u001b[39miloc[valid_index], cat_features \u001b[39m=\u001b[39m cat)\n\u001b[1;32m---> 28\u001b[0m     model_CAT\u001b[39m.\u001b[39;49mfit(train_pool, eval_set\u001b[39m=\u001b[39;49mvalid_pool)\n\u001b[0;32m     29\u001b[0m     scores\u001b[39m.\u001b[39mappend(model_CAT\u001b[39m.\u001b[39mbest_score_[\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mAUC\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(scores)\n",
      "File \u001b[1;32mc:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py:5131\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[0;32m   5129\u001b[0m     CatBoostClassifier\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m-> 5131\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline, use_best_model,\n\u001b[0;32m   5132\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[0;32m   5133\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[0;32m   5134\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py:2357\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2353\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   2355\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[0;32m   2356\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2357\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[0;32m   2358\u001b[0m         train_pool,\n\u001b[0;32m   2359\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   2360\u001b[0m         params,\n\u001b[0;32m   2361\u001b[0m         allow_clear_pool,\n\u001b[0;32m   2362\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m   2363\u001b[0m     )\n\u001b[0;32m   2365\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2366\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32mc:\\Users\\Jose\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py:1761\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1760\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1761\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m   1762\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:4624\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4673\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Specify a search space using trial object\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 50, 300),\n",
    "        'depth': trial.suggest_int('depth', 4, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True), \n",
    "        'random_strength': trial.suggest_int('random_strength', 0, 100),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.01, 100.00, log=True), \n",
    "        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "        'od_wait': trial.suggest_int('od_wait', 10, 50),\n",
    "        'loss_function': 'Logloss',  # Binary classification\n",
    "        'eval_metric': 'AUC',  # AUC as the performance metric\n",
    "    }\n",
    "\n",
    "    model_CAT = CatBoostClassifier(**params, verbose=False)\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    scores = []\n",
    "    for train_index, valid_index in cv.split(X_train_CAT, y_train_CAT):\n",
    "        train_pool = Pool(data=X_train_CAT.iloc[train_index], label=y_train_CAT.iloc[train_index], cat_features = cat)\n",
    "        valid_pool = Pool(data=X_train_CAT.iloc[valid_index], label=y_train_CAT.iloc[valid_index], cat_features = cat)\n",
    "\n",
    "        model_CAT.fit(train_pool, eval_set=valid_pool)\n",
    "        scores.append(model_CAT.best_score_['validation']['AUC'])\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Create a study object\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Start optimization\n",
    "study.optimize(objective, n_trials=50)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[I 2023-06-17 11:51:23,335] Trial 11 finished with value: 0.9783593516221193 and parameters: {'iterations': 248, 'depth': 8, 'learning_rate': 0.07556114402524425, 'random_strength': 2, 'bagging_temperature': 1.6353270643758222, 'od_type': 'Iter', 'od_wait': 22}. Best is trial 11 with value: 0.9783593516221193.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_Cat_Opt_AllCat = {'iterations': 249, \n",
    "                       'depth': 6, \n",
    "                       'learning_rate': 0.1555748471212781, \n",
    "                       'random_strength': 58, \n",
    "                       'bagging_temperature': 87.47376677399185, \n",
    "                       'od_type': 'IncToDec', \n",
    "                       'od_wait': 27} # 0.9791965650818231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_CAT = {'iterations': 290, \n",
    "            'depth': 12, \n",
    "            'learning_rate': 0.016041043108679078, \n",
    "            'random_strength': 12, \n",
    "            'bagging_temperature': 0.014032128937182435, \n",
    "            'od_type': 'IncToDec', \n",
    "            'od_wait': 47}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1e016936610>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_CAT = CatBoostClassifier(**Best_Cat_Opt_AllCat, verbose=False)\n",
    "model_CAT.fit(X_train_CAT, y_train_CAT, cat_features = cat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  Normal dataset AUC LGBM:  0.9724129378237473\n",
      "Dataset:  Dataset With FE AUC LGBM:  0.9702258169335194\n",
      "Dataset:  Dataset With FE and FR AUC LGBM:  0.9674699971488955\n",
      "Dataset:  Dataset Ori + train no FE AUC LGBM:  0.9784126460666145\n",
      "Dataset:  Dataset Ori + train with FE AUC LGBM:  0.9755411959281259\n"
     ]
    }
   ],
   "source": [
    "for name, model in Train_sets.items():\n",
    "\n",
    "    X_train, X_test  = dataset(model[0], model[1], model[2], model[3])\n",
    "    y_train = X_train['Machine_failure']\n",
    "    X_train = X_train.drop(['Machine_failure'], axis=1)\n",
    "\n",
    "    # Use the function\n",
    "    AUC = ROC_AUC(model_XGB, X_train, y_train)\n",
    "    print(\"Dataset: \", name, \"AUC LGBM: \", AUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC LGBM:  0.9751743189440938\n",
      "AUC XGB:  0.9784126460666145\n"
     ]
    }
   ],
   "source": [
    "# Use the function\n",
    "AUC_LGBM = ROC_AUC(model_LGBM, X_train, y_train)\n",
    "print(\"AUC LGBM: \", AUC_LGBM)\n",
    "\n",
    "AUC_XGB = ROC_AUC(model_XGB, X_train, y_train)\n",
    "print(\"AUC XGB: \", AUC_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC CAT:  0.977913617834177\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Repeated Stratified K Fold\n",
    "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=42)\n",
    "\n",
    "# Create an empty list to store the Out-of-Fold (OOF) predictions\n",
    "oof_preds = np.zeros(X_train_CAT.shape[0])\n",
    "\n",
    "\n",
    "# Loop through each split\n",
    "for train_index, valid_index in rskf.split(X_train_CAT.astype(str), y_train):\n",
    "\n",
    "    # Split the data\n",
    "    X_train_fold, X_valid_fold = X_train_CAT.iloc[train_index], X_train_CAT.iloc[valid_index]\n",
    "    y_train_fold, y_valid_fold = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "    # Fit the model\n",
    "    model_CAT.fit(X_train_fold.astype(str), y_train_fold, cat_features = cat)\n",
    "\n",
    "    # Make predictions and add them to the OOF predictions\n",
    "    oof_preds[valid_index] = model_CAT.predict_proba(X_valid_fold.astype(str))[:,1]\n",
    "\n",
    "# Calculate the overall AUC\n",
    "auc = roc_auc_score(y_train, oof_preds)\n",
    "\n",
    "\n",
    "print(\"AUC CAT: \", auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC:  0.9659444919120036\n",
    "AUC:  0.9702258169335194\n",
    "AUC:  0.9605964382592067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las predicciones\n",
    "pred_lgbm = model_LGBM.predict_proba(X_train)[:,1]\n",
    "pred_xgb = model_XGB.predict_proba(X_train)[:,1]\n",
    "pred_cat = model_CAT.predict_proba(X_train_CAT.astype(str))[:,1]\n",
    "\n",
    "X_train_meta = pd.DataFrame({'lgbm': pred_lgbm, 'xgb': pred_xgb, 'cat': pred_cat, 'target': y_train})\n",
    "\n",
    "pred_lgbm_test = model_LGBM.predict_proba(X_test)[:,1]\n",
    "pred_xgb_test = model_XGB.predict_proba(X_test)[:,1]\n",
    "pred_cat_test = model_CAT.predict_proba(X_test_CAT.astype(str))[:,1]\n",
    "\n",
    "X_test_meta = pd.DataFrame({'lgbm': pred_lgbm_test, 'xgb': pred_xgb_test, 'cat': pred_cat_test})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tunning for meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_m = X_train_meta['target']\n",
    "X_train_m = X_train_meta.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:46<00:00,  8.14s/trial, best loss: -0.9923355097281873]\n",
      "Best:  {'colsample_bytree': 0.026181323096692063, 'learning_rate': 0.04207644660879477, 'max_depth': 6.0, 'min_child_samples': 26.0, 'n_estimators': 190.0, 'num_leaves': 21.0, 'reg_alpha': 0.4910393820375527, 'reg_lambda': 0.6246168616161446, 'subsample': 0.41742010255980644}\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import lightgbm as lgb\n",
    "# hp \n",
    "from hyperopt import hp, tpe, STATUS_OK, Trials, fmin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "\n",
    "space ={\n",
    "    'max_depth': hp.quniform('max_depth', 6, 18, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 100, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.01, 0.3),\n",
    "    'subsample': hp.uniform('subsample', 0.4, 1.0),\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 400, 10),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 100, 1),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.15, 0.5),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.3, 1.0)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def objective(space):\n",
    "    model_LGBM_meta = lgb.LGBMClassifier(max_depth = int(space['max_depth']),\n",
    "                             learning_rate = space['learning_rate'],\n",
    "                             num_leaves = int(space['num_leaves']),\n",
    "                             n_estimators = int(space['n_estimators']),\n",
    "                             colsample_bytree = space['colsample_bytree'],\n",
    "                             subsample = space['subsample'],\n",
    "                             is_unbalance = True,\n",
    "                             min_child_samples = int(space['min_child_samples']),\n",
    "                             reg_alpha = space['reg_alpha'],\n",
    "                             reg_lambda = space['reg_lambda'])\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "    y_pred_proba = cross_val_predict(model_LGBM_meta, X_train_m, y_train_m, cv=cv, method='predict_proba')[:,1]\n",
    "    auc = roc_auc_score(y_train_m, y_pred_proba)\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n",
    "\n",
    "# Run the algorithm\n",
    "trials = Trials()\n",
    "best_LGBM_meta = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Best: \", best_LGBM_meta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_meta_LGBM =  {'colsample_bytree': 0.22071462239650944, \n",
    "                'learning_rate': 0.0071184882183990906, \n",
    "                'max_depth': 12, \n",
    "                'min_child_samples': 91, \n",
    "                'n_estimators': 180, \n",
    "                'num_leaves': 20, \n",
    "                'reg_alpha': 0.34357926147986606, \n",
    "                'reg_lambda': 0.9332514748078644, \n",
    "                'subsample': 0.9562001151634876} # 0.988\n",
    "\n",
    "best_LGBM_meta = {'colsample_bytree': 0.026181323096692063, \n",
    "                  'learning_rate': 0.04207644660879477, \n",
    "                  'max_depth': 6, \n",
    "                  'min_child_samples': 26, \n",
    "                  'n_estimators': 190, \n",
    "                  'num_leaves': 21, \n",
    "                  'reg_alpha': 0.4910393820375527, \n",
    "                  'reg_lambda': 0.6246168616161446, \n",
    "                  'subsample': 0.41742010255980644} # 0.9923355097281873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC META LGBM tunned:  0.9893425917870007\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(colsample_bytree=0.026181323096692063,\n",
       "               learning_rate=0.04207644660879477, max_depth=6,\n",
       "               min_child_samples=26, n_estimators=190, num_leaves=21,\n",
       "               reg_alpha=0.4910393820375527, reg_lambda=0.6246168616161446,\n",
       "               subsample=0.41742010255980644)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=0.026181323096692063,\n",
       "               learning_rate=0.04207644660879477, max_depth=6,\n",
       "               min_child_samples=26, n_estimators=190, num_leaves=21,\n",
       "               reg_alpha=0.4910393820375527, reg_lambda=0.6246168616161446,\n",
       "               subsample=0.41742010255980644)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.026181323096692063,\n",
       "               learning_rate=0.04207644660879477, max_depth=6,\n",
       "               min_child_samples=26, n_estimators=190, num_leaves=21,\n",
       "               reg_alpha=0.4910393820375527, reg_lambda=0.6246168616161446,\n",
       "               subsample=0.41742010255980644)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_meta_LGBM = lgb.LGBMClassifier(**best_LGBM_meta)\n",
    "\n",
    "AUC_META_LGBM_tunned = ROC_AUC(model_meta_LGBM, X_train_m, y_train_m)\n",
    "print(\"AUC META LGBM tunned: \", AUC_META_LGBM_tunned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_meta_LGBM.fit(X_train_m, y_train_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_meta_LGBM.predict_proba(X_test_meta)[:,1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_meta_LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_analysis = model.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_analysis = X_train.copy()\n",
    "X_train_analysis['Machine_failure'] = y_train\n",
    "X_train_analysis['Machine_failure_pred'] = y_pred_analysis\n",
    "X_train_analysis['Error'] = X_train_analysis['Machine_failure'] - X_train_analysis['Machine_failure_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_analysis.to_csv('data/X_train_analysis.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_submission = pd.read_csv('input/test.csv') # Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission to a file\n",
    "# Rounded pred increases the score\n",
    "submission = pd.DataFrame({'id': X_test_submission['id'], 'Machine failure': y_pred})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
